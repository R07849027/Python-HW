{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主題：以CNN做表情辨認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 組員：張毓倩、江育誠、黃涱煜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 介紹：\n",
    "   \n",
    "    本資料來自於 kaggle 網站(來源在文末連結)，共有兩萬八千張左右48x48 pixel的圖片，每一張圖片皆有表情label，而每張圖片都只會有一種表情。總共有七種可能的表情（0：生氣, 1：厭惡, 2：恐懼, 3：高興, 4：難過, 5：驚訝, 6：中立(難以區分為前六種的表情))。此外，還有七千筆 testing data 是沒有 label 的，因此我們會將兩萬八千張有 label 的資料以 CNN 建立出預測模型後，再預測出 testing data 的 label。最後上傳至 kaggle 查看預測準確率。\n",
    "    (https://www.kaggle.com/c/ml2019spring-hw3/kernels)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 實作方法：\n",
    "\n",
    "    1. Training and validation\n",
    "    \n",
    "        將兩萬八千筆有 label 的資料切割成 training set 與 validation set，以 CNN 的方式建模並以 validation set 的 accuracy 當作預測指標。當 accuracy 符合預期後，以同樣的 model 參數再用全部兩萬八千筆資料來 fit，得到最終的 model。\n",
    "    \n",
    "    2. Testing\n",
    "    \n",
    "        將最終的 model 預測那七千筆 testing data 的 label，上傳至 kaggle 查看預測準確率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 專案排程\n",
    "\n",
    "    5/26設計CNN模型\n",
    "    \n",
    "    5/28使用較少量資料跑模型\n",
    "    \n",
    "    6/2比較各模型結果並選定模型\n",
    "    \n",
    "    6/3~6/6彙整並撰寫期末專題報告"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目前進度：以部分資料建立模型並評估\n",
    "\n",
    "    從兩萬八千筆有 label 的資料中，挑選前 5000 筆當作 training set，最後 1500 筆做 testing set，以少量資料初步評估模型的預測率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, \\\n",
    "      Activation, BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "   all_features = []\n",
    "   all_labels = []\n",
    "   count = 0\n",
    "   test_num = 5000\n",
    "   with open(\"train.csv\", 'r', encoding='big5') as file:\n",
    "      file.readline()\n",
    "      all_rows = csv.reader(file, delimiter=',')\n",
    "      for row in all_rows:\n",
    "         if count == test_num:\n",
    "            break\n",
    "         all_labels.append(row[0])\n",
    "         all_features.append(row[1].split(' '))\n",
    "         count += 1\n",
    "\n",
    "   all_features = np.array(all_features).astype('float')\n",
    "   all_features = all_features.reshape(test_num, 48, 48, 1)\n",
    "\n",
    "   all_labels = np.array(all_labels).astype('float')\n",
    "   all_labels = np_utils.to_categorical(all_labels, 7)\n",
    "\n",
    "   return all_features, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data2():\n",
    "   all_features = []\n",
    "   all_labels = []\n",
    "   count = 0\n",
    "   test_num = 1500\n",
    "   with open(\"validation.csv\", 'r', encoding='big5') as file:\n",
    "      file.readline()\n",
    "      all_rows = csv.reader(file, delimiter=',')\n",
    "      for row in all_rows:\n",
    "         if count == test_num:\n",
    "            break\n",
    "         all_labels.append(row[0])\n",
    "         all_features.append(row[1].split(' '))\n",
    "         count += 1\n",
    "\n",
    "   all_features = np.array(all_features).astype('float')\n",
    "   all_features = all_features.reshape(test_num, 48, 48, 1)\n",
    "\n",
    "   all_labels = np.array(all_labels).astype('float')\n",
    "   all_labels = np_utils.to_categorical(all_labels, 7)\n",
    "\n",
    "   return all_features, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(x_train):\n",
    "   model = Sequential()\n",
    "   num_classes = 7\n",
    "\n",
    "   model.add(Conv2D(64, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "   model.add(BatchNormalization())\n",
    "   model.add(Activation('relu'))\n",
    "\n",
    "   model.add(Conv2D(64, (3, 3)))\n",
    "   model.add(BatchNormalization())\n",
    "   model.add(Activation('relu'))\n",
    "\n",
    "   model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "   model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "   model.add(BatchNormalization())\n",
    "   model.add(Activation('relu'))\n",
    "\n",
    "   model.add(Conv2D(32, (3, 3)))\n",
    "   model.add(BatchNormalization())\n",
    "   model.add(Activation('relu'))\n",
    "\n",
    "   model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "   model.add(Flatten())\n",
    "\n",
    "   model.add(Dense(16))\n",
    "   model.add(BatchNormalization())\n",
    "   model.add(Activation('relu'))\n",
    "\n",
    "   model.add(Dense(64))\n",
    "   model.add(BatchNormalization())\n",
    "   model.add(Activation('relu'))\n",
    "\n",
    "   model.add(Dense(num_classes))\n",
    "   model.add(Activation('softmax'))\n",
    "\n",
    "   return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load data ##\n",
    "X_train, Y_train = load_data()\n",
    "\n",
    "## normalize ##\n",
    "X_train = X_train / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load data ##\n",
    "X_test, Y_test = load_data()\n",
    "\n",
    "## normalize ##\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 46, 46, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 46, 46, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 46, 46, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 23, 23, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 23, 23, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 21, 21, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 21, 21, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                51216     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 455       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 119,127\n",
      "Trainable params: 118,583\n",
      "Non-trainable params: 544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## main ##\n",
    "model = build_model(X_train)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam(lr=0.001),\n",
    "               metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "5000/5000 [==============================] - 176s 35ms/step - loss: 1.6780 - acc: 0.3644 - val_loss: 1.8721 - val_acc: 0.2564\n",
      "Epoch 2/20\n",
      "5000/5000 [==============================] - 177s 35ms/step - loss: 1.4875 - acc: 0.4534 - val_loss: 1.8541 - val_acc: 0.2620\n",
      "Epoch 3/20\n",
      "5000/5000 [==============================] - 176s 35ms/step - loss: 1.3312 - acc: 0.5140 - val_loss: 1.8415 - val_acc: 0.2598\n",
      "Epoch 4/20\n",
      "5000/5000 [==============================] - 177s 35ms/step - loss: 1.1977 - acc: 0.5752 - val_loss: 1.8515 - val_acc: 0.2298\n",
      "Epoch 5/20\n",
      "5000/5000 [==============================] - 176s 35ms/step - loss: 1.0664 - acc: 0.6254 - val_loss: 1.8690 - val_acc: 0.2268\n",
      "Epoch 6/20\n",
      "5000/5000 [==============================] - 177s 35ms/step - loss: 0.9500 - acc: 0.6758 - val_loss: 1.8813 - val_acc: 0.1920\n",
      "Epoch 7/20\n",
      "5000/5000 [==============================] - 176s 35ms/step - loss: 0.8144 - acc: 0.7338 - val_loss: 1.9799 - val_acc: 0.1804\n",
      "Epoch 8/20\n",
      "5000/5000 [==============================] - 176s 35ms/step - loss: 0.6873 - acc: 0.7910 - val_loss: 2.0252 - val_acc: 0.2346\n",
      "Epoch 9/20\n",
      "5000/5000 [==============================] - 176s 35ms/step - loss: 0.5690 - acc: 0.8334 - val_loss: 2.0886 - val_acc: 0.1994\n",
      "Epoch 10/20\n",
      "5000/5000 [==============================] - 176s 35ms/step - loss: 0.4403 - acc: 0.8884 - val_loss: 2.2221 - val_acc: 0.2230\n",
      "Epoch 11/20\n",
      "5000/5000 [==============================] - 176s 35ms/step - loss: 0.3430 - acc: 0.9156 - val_loss: 2.4673 - val_acc: 0.1856\n",
      "Epoch 12/20\n",
      "5000/5000 [==============================] - 176s 35ms/step - loss: 0.2707 - acc: 0.9418 - val_loss: 2.8471 - val_acc: 0.1826\n",
      "Epoch 13/20\n",
      "5000/5000 [==============================] - 176s 35ms/step - loss: 0.2189 - acc: 0.9528 - val_loss: 2.7208 - val_acc: 0.1960\n",
      "Epoch 14/20\n",
      "5000/5000 [==============================] - 176s 35ms/step - loss: 0.1673 - acc: 0.9670 - val_loss: 2.6986 - val_acc: 0.2408\n",
      "Epoch 15/20\n",
      "5000/5000 [==============================] - 179s 36ms/step - loss: 0.1241 - acc: 0.9772 - val_loss: 3.0828 - val_acc: 0.2176\n",
      "Epoch 16/20\n",
      "5000/5000 [==============================] - 176s 35ms/step - loss: 0.0897 - acc: 0.9858 - val_loss: 2.5900 - val_acc: 0.2912\n",
      "Epoch 17/20\n",
      "5000/5000 [==============================] - 176s 35ms/step - loss: 0.0663 - acc: 0.9910 - val_loss: 2.4611 - val_acc: 0.3082\n",
      "Epoch 18/20\n",
      "5000/5000 [==============================] - 176s 35ms/step - loss: 0.0517 - acc: 0.9936 - val_loss: 2.4244 - val_acc: 0.3066\n",
      "Epoch 19/20\n",
      "5000/5000 [==============================] - 176s 35ms/step - loss: 0.0389 - acc: 0.9950 - val_loss: 2.3140 - val_acc: 0.3368\n",
      "Epoch 20/20\n",
      "5000/5000 [==============================] - 176s 35ms/step - loss: 0.0302 - acc: 0.9966 - val_loss: 2.3882 - val_acc: 0.3422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22282534c88>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=256, epochs=20,validation_data = (X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model-5000.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目前進度結論：\n",
    "\n",
    "    在模型參數高達10萬多個下，Training set accuracy = 99.66%，但 testing set accuracy = 34.22%，因此以目前看來該模型的設計可能有 overfitting 的現象，之後設計模型的方向會以避免過度擬合的角度去發展。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
